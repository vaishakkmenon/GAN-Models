# full_script.py

import os
import sys

# Add root path to sys for module resolution
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, ConcatDataset
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.amp import autocast, GradScaler
from torchvision import datasets, transforms
from torchvision.utils import save_image

from PIL import Image
import matplotlib.pyplot as plt

# =============================
# Generator Model
# =============================
class Generator(nn.Module):
    # Latent_dim = Latent Dimension: Input noise vector
    # Img_shape = shape of image: 28x28
    def __init__(self, latent_dim, img_shape):
        super(Generator, self).__init__()

        # First fully connected layer to convert from 100 to 128
        # Lifts noise vector to feature space representing data
        self.fc1 = nn.Linear(latent_dim, 128)

        # Second fully connected layer to convert from 128 to 256
        # Increases model capacity to learn complex features
        self.fc2 = nn.Linear(128, 256)

        # Normalizes the output of a layer so the next one learns better
        self.bn = nn.BatchNorm1d(256)

        # Final fully connected layer to convert from 256 to 768 (28 * 28)
        self.fc3 = nn.Linear(256, img_shape)

        # Keeps values positive and allows for complex patterns to be learned
        self.relu = nn.ReLU(inplace=True)

        # Scales all output values to be between -1 and 1
        self.tanh = nn.Tanh()

    def forward(self, z):
        # Pass noise into linear layer for learned features
        x = self.fc1(z)
        # Introduce non-linearity
        x = self.relu(x)

        # Increase learned features
        x = self.fc2(x)
        # Normalize output thus far; Training stabilization, better gradient flow, normalized input
        x = self.bn(x)
        # Learn non-linear features from normalized data
        x = self.relu(x)

        # Final output layer to turn features into real-valued pixel data
        x = self.fc3(x)
        # Squash all output values to be between -1 and 1
        out = self.tanh(x)

        return out

# =============================
# Discriminator Model
# =============================
class Discriminator(nn.Module):
    def __init__(self, img_shape, use_sigmoid=True):
        super(Discriminator, self).__init__()

        # First fully connected layer: takes 784 pixels -> 256 features
        self.fc1 = nn.Linear(img_shape, 256)
        # Second fully connected layer: takes 256 features -> 128 features
        self.fc2 = nn.Linear(256, 128)
        # Third fully connected layer: takes 128 features to 1 (real or fake)
        self.fc3 = nn.Linear(128, 1)

        # Nonlinear activation that avoids dead neurons.
        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)

        # Squashes output to [0, 1] to represent â€œrealness"
        self.sigmoid = nn.Sigmoid()

        self.use_sigmoid = use_sigmoid

    def forward(self, x):
        # Pass the image through each layer, use leakyReLU for non-linear activation, 
        # and sigmoid for final output value
        # Sigmoid may be ignored based on loss function
        x = self.fc1(x)
        x = self.leaky_relu(x)

        x = self.fc2(x)
        x = self.leaky_relu(x)

        x = self.fc3(x)
        if self.use_sigmoid:
            x = self.sigmoid(x)

        return x

# =============================
# Preprocessing
# =============================
def resolve_data_path(provided_path):
    """
    Resolves the absolute path to the dataset, ensuring 'data/' is alongside 'models/' directory.
    """
    models_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    data_path = os.path.join(models_dir, "data", provided_path)
    os.makedirs(os.path.dirname(data_path), exist_ok=True)
    print(f"[INFO] Resolved dataset path: {data_path}")
    return data_path

def load_mnist_full(provided_path="mnist/", batch_size=64, num_workers=8):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    data_path = resolve_data_path(provided_path)

    train_dataset = datasets.MNIST(data_path, train=True, transform=transform, download=True)
    test_dataset = datasets.MNIST(data_path, train=False, transform=transform, download=True)

    full_dataset = ConcatDataset([train_dataset, test_dataset])

    print("[INFO] MNIST Combined Train + Test Data Loaded")
    return full_dataset

# =============================
# Training Function (from train.py with all comments preserved)
# =============================
# --- Config ---
latent_dim = 100
img_shape = 28 * 28
batch_size = 256
epochs = 100
G_UPDATES_PER_D = 2  # Train generator more times than discriminator

def train(rank, world_size, save_dir="generated-latest", checkpoint_dir="checkpoints-latest"):
    # Initializes the distributed training process
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)
    device = torch.device(f"cuda:{rank}")
    print(f"[INFO] Training on device: {device}")

    # Load and distribute dataset across multiple GPUs
    dataset = load_mnist_full()
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)
    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True)

    # Initialize models and wrap them with DDP
    G = DDP(Generator(latent_dim, img_shape).to(device), device_ids=[rank])
    D = DDP(Discriminator(img_shape, use_sigmoid=False).to(device), device_ids=[rank])

    # Set up optimizers for both models
    # Learning Rate = 0.0002;
    # Betas (0.5): Momentum term
    # Betas (0.999): Controls how quickly the optimizer adapts learning rates
    optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))

    # Cosine annealing learning rate schedulers
    scheduler_G = CosineAnnealingLR(optimizer_G, T_max=epochs, eta_min=1e-5)
    scheduler_D = CosineAnnealingLR(optimizer_D, T_max=epochs, eta_min=1e-5)

    # AMP scalers for mixed precision training on RTX GPUs
    scaler_G = GradScaler('cuda')
    scaler_D = GradScaler('cuda')

    # Lists to store values for plotting at the end
    lr_history_G, lr_history_D, g_losses, d_losses, grad_norms_G= [], [], [], [], []

    for epoch in range(epochs):
        # Ensures shuffling across epochs for DistributedSampler
        sampler.set_epoch(epoch)  
        print(f"[INFO] Starting epoch {epoch} on rank {rank}")

        total_g_loss = 0.0
        total_d_loss = 0.0
        total_grad_norm = 0.0
        grad_norm_count = 0

        for batch_idx, (imgs, _) in enumerate(train_loader):
            imgs = imgs.view(-1, img_shape).to(device)  # Flatten and move images to device
            bs = imgs.size(0)

            # Real and fake labels
            valid = torch.ones(bs, 1, device=device)
            fake = torch.zeros(bs, 1, device=device)

            # ======================
            #  Train Discriminator
            # ======================

            # Random noise creation for generator to use
            z = torch.randn(bs, latent_dim, device=device)
            with autocast(device_type='cuda'):
                # Create fake images based on random noise
                # detach(): Stops backpropagation for generator so gradients are not updated
                fake_imgs = G(z).detach()
                # Encourage the discriminator to output large positive values for real images.
                real_loss = torch.mean(torch.relu(1.0 - D(imgs)))
                # Encourage the discriminator to output large negative values for fake images.
                fake_loss = torch.mean(torch.relu(1.0 + D(fake_imgs)))
                # Combine losses into overall loss; Balances contribtion of real and fake
                d_loss = (real_loss + fake_loss) / 2

            # Clear out any old gradients from the previous update
            # This prevents gradient accumulation across batches
            optimizer_D.zero_grad()

            # Scale the discriminator loss to amplify gradients for float16 precision
            # Helps prevent underflow and ensures gradient signal isn't lost
            scaler_D.scale(d_loss).backward()

            # Unscale the gradients and perform the optimizer step only if gradients are finite
            scaler_D.step(optimizer_D)

            # Adjust the scale factor for future steps based on gradient stability
            scaler_D.update()

            # ======================
            #  Train Generator
            # ======================
            
            g_loss = 0.0
            for _ in range(G_UPDATES_PER_D):
                # Random noise creation for generator to use
                z = torch.randn(bs, latent_dim, device=device)
                with autocast(device_type='cuda'):
                    # Generate fake images
                    gen_imgs = G(z)
                    # Pass the generated images into the Discriminator, trying to fool Discriminator
                    g_loss_step = -torch.mean(D(gen_imgs))

                # Clear out any old gradients from the previous update
                optimizer_G.zero_grad()
                # Scale the generator loss to amplify gradients for float16 precision
                # Helps prevent underflow and ensures gradient signal isn't lost
                scaler_G.scale(g_loss_step).backward()

                # === Gradient Norm Logging for G ===
                total_norm = 0.0
                for p in G.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.data.norm(2)
                        total_norm += param_norm.item() ** 2
                total_norm = total_norm ** 0.5

                if rank == 0 and batch_idx % 100 == 0:
                    print(f"[Epoch {epoch}] [Batch {batch_idx}] G Grad Norm: {total_norm:.4f}")

                total_grad_norm += total_norm
                grad_norm_count += 1

                scaler_G.step(optimizer_G)
                scaler_G.update()

                g_loss += g_loss_step.item()

            g_loss /= G_UPDATES_PER_D
            total_g_loss += g_loss
            total_d_loss += d_loss.item()

            if batch_idx % 100 == 0 and rank == 0:
                print(f"[Epoch {epoch}/{epochs}] [Batch {batch_idx}/{len(train_loader)}] "
                    f"[D loss: {d_loss.item():.4f}] [G loss: {g_loss:.4f}]")

        scheduler_G.step()
        scheduler_D.step()

        # ======================
        #  Save Output & Checkpoints
        # ======================
        if rank == 0:
            avg_g_loss = total_g_loss / len(train_loader)
            avg_d_loss = total_d_loss / len(train_loader)
            avg_grad_norm = total_grad_norm / grad_norm_count if grad_norm_count > 0 else 0.0
            
            g_losses.append(avg_g_loss)
            d_losses.append(avg_d_loss)
            grad_norms_G.append(avg_grad_norm)
            
            lr_history_G.append(scheduler_G.get_last_lr()[0])
            lr_history_D.append(scheduler_D.get_last_lr()[0])

            G.eval()
            with torch.no_grad():
                z = torch.randn(25, latent_dim, device=device)
                samples = G.module(z).view(-1, 1, 28, 28)
                save_image(samples, f"{save_dir}/epoch_{epoch}.png", nrow=5, normalize=True)
            G.train()

            torch.save(G.module.state_dict(), os.path.join(checkpoint_dir, f"generator_epoch_{epoch}.pth"))
            torch.save(D.module.state_dict(), os.path.join(checkpoint_dir, f"discriminator_epoch_{epoch}.pth"))

    # Clean up the process group
    print(f"[INFO] Finished training on rank {rank}, cleaning up...")
    dist.destroy_process_group()

    if rank == 0:
        plt.figure()
        plt.plot(g_losses, label='G Loss')
        plt.plot(d_losses, label='D Loss')
        plt.title('Generator and Discriminator Loss Per Epoch')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        plt.savefig("loss_plot.png")

        plt.figure()
        plt.plot(lr_history_G, label='G LR')
        plt.plot(lr_history_D, label='D LR')
        plt.title('Cosine Annealing Learning Rate Schedule')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.legend()
        plt.grid(True)
        plt.savefig("lr_schedule_plot.png")
        
        # === Plot Gradient Norms ===
        plt.figure()
        plt.plot(grad_norms_G, label='G Gradient Norm')
        plt.title('Generator Gradient Norm Per Epoch')
        plt.xlabel('Epoch')
        plt.ylabel('L2 Norm')
        plt.legend()
        plt.grid(True)
        plt.savefig("grad_norm_plot.png")
        
    print("[INFO] Program Complete!")

# --- Main Entry ---
if __name__ == "__main__":
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    world_size = 4
    print("[INFO] Launching Training")
    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)